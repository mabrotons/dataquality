# -*- coding: utf-8 -*-
"""DQ_weather_univariate_v1.0.ipynb

Automatically generated by Colaboratory.

Original file is located at

##     Explore dataset

**Loading dataset** 

Dataset used with temperature information is based on Weather Dataset (https://www.kaggle.com/datasets/muthuj7/weather-dataset) and is stored in a public github repository.
"""

import pandas as pd

#based on dataset https://www.kaggle.com/datasets/muthuj7/weather-dataset
#Import dataset dates from github
url = 'https://raw.githubusercontent.com/mabrotons/datasets/master/weatherHistory_2015_1D.csv'

df = pd.read_csv(url, index_col=False, parse_dates=['Formatted Date'])

df.sort_values(by='Formatted Date', inplace = True)
df.set_index('Formatted Date', inplace = True)

print("Data frame info: ") 
print(df.info())

print("\nData frame head: ") 
print(df.head())

print("\nDescribe: ") 
print(df.describe())

"""As a first step on the analysis, we will build a graph to represent all the data included in the dataset in order to have a first view of the data (time line) and its distribution, max, min and mean values."""

import matplotlib.pyplot as plt

fig, axes =plt.subplots(2, 1, figsize=(18, 15))

axes[0].plot(df.index, df['Temperature (C)'], label='Temperature C', linewidth=1)

#add horizontal lines at mean value of x
mean_temp = df['Temperature (C)'].mean()
max_temp = df['Temperature (C)'].max()
min_temp = df['Temperature (C)'].min()

axes[0].axhline(y=mean_temp, color='green', linewidth=1, label='mean', linestyle='dashed')
axes[0].axhline(y=max_temp, color='red', linewidth=1, label='max', linestyle='dashed')
axes[0].axhline(y=min_temp, color='blue', linewidth=1, label='min', linestyle='dashed')
axes[1].hist(df['Temperature (C)'], bins=50, edgecolor='black')

axes[0].title.set_text('Temperature (C) - time serie')
axes[1].title.set_text('Temperature (C) - density distribution')

axes[0].tick_params('x', labelrotation=30)
axes[1].tick_params('x', labelrotation=30)

axes[0].legend()

plt.show()

"""In the case, as we can appreciate in the time series chart, we are working with a seasonal time series, where there is a clear pattern of year's seasons. By the other hand, we can see that temperature datapoint distribution  is very close to a normal distribution.

The seasonal characteristic of this time series means that the global statistical functions such as the mean, the maximum and the minimum, being at a global level, only provide data at that level but distorting the data in each of the seasons of each year. For this reason, it would be interesting to analyze this data set using maximum, minimum and average metrics, aggregating the information at a monthly, weekly and daily level.

The three graphs will show us in three different time scales and through the red areas (temperatures above the average for the period) and the blue areas (temperatures below the average for the period) the different temperature ranges recorded throughout of the time line.
"""

fig, axes =plt.subplots(3, 1, figsize=(20, 25))

month_agg = df.groupby(df.index.to_period("M"))['Temperature (C)'].agg(['min', 'mean', 'max', 'std']) #Monthly
axes[0].plot(month_agg.index.astype('datetime64'), month_agg['mean'], color='black', label='mean') 
axes[0].fill_between(month_agg.index.astype('datetime64'), month_agg['max'], month_agg['mean'], alpha=0.2, color='red', label='max area')
axes[0].fill_between(month_agg.index.astype('datetime64'), month_agg['min'], month_agg['mean'], alpha=0.2, color='blue', label='min area')
axes[0].legend()

week_agg = df.groupby(df.index.to_period("W"))['Temperature (C)'].agg(['min', 'mean', 'max', 'std']) #Monthly
axes[1].plot(week_agg.index.astype('datetime64'), week_agg['mean'], color='black', label='mean') 
axes[1].fill_between(week_agg.index.astype('datetime64'), week_agg['max'], week_agg['mean'], alpha=0.2, color='red', label='max area')
axes[1].fill_between(week_agg.index.astype('datetime64'), week_agg['min'], week_agg['mean'], alpha=0.2, color='blue', label='min area')
axes[1].legend()

day_agg = df.groupby(df.index.to_period("D"))['Temperature (C)'].agg(['min', 'mean', 'max', 'std']) #Monthly
axes[2].plot(day_agg.index.astype('datetime64'), day_agg['mean'], color='black', label='mean') 
axes[2].fill_between(day_agg.index.astype('datetime64'), day_agg['max'], day_agg['mean'], alpha=0.2, color='red', label='max area')
axes[2].fill_between(day_agg.index.astype('datetime64'), day_agg['min'], day_agg['mean'], alpha=0.2, color='blue', label='min area')
axes[2].legend()

axes[0].title.set_text('Temperature (C) - Monthly')
axes[1].title.set_text('Temperature (C) - Weekly')
axes[2].title.set_text('Temperature (C) - Daily')

axes[0].tick_params('x', labelrotation=30)
axes[1].tick_params('x', labelrotation=30)
axes[2].tick_params('x', labelrotation=30)

plt.show()

"""##     Detecting outliers with basic statistics
The z-score is a calculation that measure how many standard deviations a value is far away from the mean, and the probability of data to be unusual in a distribution. Datapoints values above the mean will have positive standard scores and datapoints values below the mean have negative standard scores. 

Z = (x – μ) / σ

where Z is the score, x is the value to calculate the score, μ is the mean and σ is the standard deviation.

- if a z-score returned is lower than 1 shoud be a normal data value
- if a z-score returned is larger than 1 and lower than 3, could be an error
- if a z-score returned is larger than 3 should be an error
"""

std_temp = df['Temperature (C)'].std()
df['zscore_global'] = [(a-mean_temp)/std_temp for a in df['Temperature (C)']] 
print("\nMean deviation of temperatures: " + str(mean_temp))
print("Standard deviation of temperatures: " + str(std_temp))

df['zscore_global_class'] = ''
df.loc[(df['zscore_global'] <= 1) & (df['zscore_global'] >= -1), 'zscore_global_class'] = 'good zscore'
df.loc[((df['zscore_global'] > 1) & (df['zscore_global'] <= 3)) | ((df['zscore_global'] < -1) & (df['zscore_global'] >= -3)), 'zscore_global_class'] = 'regular zscore'
df.loc[(df['zscore_global'] > 3) | (df['zscore_global'] < -3), 'zscore_global_class'] = 'bad zscore'
 
print("\nTotal registers: " + str(len(df)))
print("z-score <1 registers: " + str(len(df.loc[df['zscore_global_class'] == 'good zscore'])))
print("z-score >1 & <3 registers: " + str(len(df.loc[df['zscore_global_class'] == 'regular zscore'])))
print("z-score >3 registers: " + str(len(df.loc[df['zscore_global_class'] == 'bad zscore'])))

"""Let's build a graph printing this results labeling with its score calculated."""

import plotly.express as pltx

color_discrete_map = {'regular zscore': 'orange', 'good zscore': 'green', 'bad zscore': 'red'}
fx = pltx.scatter(df.reset_index(), 'Formatted Date', 'Temperature (C)', color='zscore_global_class', color_discrete_map=color_discrete_map)
fx.update_xaxes (rangeslider_visible=True,)
fx.update_traces(marker_size=3)
fx.show()

print('\n Zscore >3 temperatures:')
print(df.loc[df['zscore_global_class'] == 'bad zscore'])

"""But since this is a time series with seasonality pattern, we can carry out the same exercise by applying the z-score with the aggregated data at a monthly, weekly and daily level, where mean and standard deviation will be calculated.

Monthly:
"""

#adding a new column with date to be used like an index after merge
df['date'] = df.index

merged_inner_month = pd.merge(left=df,right=month_agg, left_on=df.index.to_period("M"), right_on=month_agg.index)

merged_inner_month['zscore_agg'] = (merged_inner_month['Temperature (C)']-merged_inner_month['mean'])/merged_inner_month['std'] 

merged_inner_month['zscore_agg_class'] = ''
merged_inner_month.loc[(merged_inner_month['zscore_agg'] <= 1) & (merged_inner_month['zscore_agg'] >= -1), 'zscore_agg_class'] = 'good zscore'
merged_inner_month.loc[((merged_inner_month['zscore_agg'] > 1) & (merged_inner_month['zscore_agg'] <= 3)) | ((merged_inner_month['zscore_agg'] < -1) & (merged_inner_month['zscore_agg'] >= -3)), 'zscore_agg_class'] = 'regular zscore'
merged_inner_month.loc[(merged_inner_month['zscore_agg'] > 3) | (merged_inner_month['zscore_agg'] < -3), 'zscore_agg_class'] = 'bad zscore'

print("\nTotal merged registers: " + str(len(merged_inner_month)))
print("z-score <1 registers: " + str(len(merged_inner_month.loc[merged_inner_month['zscore_agg_class'] == 'good zscore'])))
print("z-score >1 & <3 registers: " + str(len(merged_inner_month.loc[merged_inner_month['zscore_agg_class'] == 'regular zscore'])))
print("z-score >3 registers: " + str(len(merged_inner_month.loc[merged_inner_month['zscore_agg_class'] == 'bad zscore'])))

color_discrete_map = {'regular zscore': 'orange', 'good zscore': 'green', 'bad zscore': 'red'}
fx = pltx.scatter(merged_inner_month.reset_index(), 'date', 'Temperature (C)', color='zscore_agg_class', color_discrete_map = color_discrete_map)
fx.update_xaxes (rangeslider_visible=True,)
fx.update_traces(marker_size=3)
fx.show()

print('\n Zscore >3 temperatures:')
print(merged_inner_month.loc[merged_inner_month['zscore_agg_class'] == 'bad zscore'])

"""
Weekly:"""

merged_inner_week = pd.merge(left=df,right=week_agg, left_on=df.index.to_period("W"), right_on=week_agg.index)

merged_inner_week['zscore_agg'] = (merged_inner_week['Temperature (C)']-merged_inner_week['mean'])/merged_inner_week['std'] 


merged_inner_week['zscore_agg_class'] = ''
merged_inner_week.loc[(merged_inner_week['zscore_agg'] <= 1) & (merged_inner_week['zscore_agg'] >= -1), 'zscore_agg_class'] = 'good zscore'
merged_inner_week.loc[((merged_inner_week['zscore_agg'] > 1) & (merged_inner_week['zscore_agg'] <= 3)) | ((merged_inner_week['zscore_agg'] < -1) & (merged_inner_week['zscore_agg'] >= -3)), 'zscore_agg_class'] = 'regular zscore'
merged_inner_week.loc[(merged_inner_week['zscore_agg'] > 3) | (merged_inner_week['zscore_agg'] < -3), 'zscore_agg_class'] = 'bad zscore'

print("\nTotal merged registers: " + str(len(merged_inner_week)))
print("z-score <1 registers: " + str(len(merged_inner_week.loc[merged_inner_week['zscore_agg_class'] == 'good zscore'])))
print("z-score >1 & <3 registers: " + str(len(merged_inner_week.loc[merged_inner_week['zscore_agg_class'] == 'regular zscore'])))
print("z-score >3 registers: " + str(len(merged_inner_week.loc[merged_inner_week['zscore_agg_class'] == 'bad zscore'])))


fx = pltx.scatter(merged_inner_week.reset_index(), 'date', 'Temperature (C)', color='zscore_agg_class', color_discrete_map = color_discrete_map)
fx.update_xaxes (rangeslider_visible=True,)
fx.update_traces(marker_size=3)
fx.show()

print('\n Zscore >3 temperatures:')
#print(merged_inner_week.loc[merged_inner_week['zscore_agg_class'] == 'bad zscore'])

"""
Daily:"""

merged_inner_day = pd.merge(left=df,right=day_agg, left_on=df.index.to_period("D"), right_on=day_agg.index)

merged_inner_day['zscore_agg'] = (merged_inner_day['Temperature (C)']-merged_inner_day['mean'])/merged_inner_day['std'] 

merged_inner_day['zscore_agg_class'] = ''
merged_inner_day.loc[(merged_inner_day['zscore_agg'] <= 1) & (merged_inner_day['zscore_agg'] >= -1), 'zscore_agg_class'] = 'good zscore'
merged_inner_day.loc[((merged_inner_day['zscore_agg'] > 1) & (merged_inner_day['zscore_agg'] <= 3)) | ((merged_inner_day['zscore_agg'] < -1) & (merged_inner_day['zscore_agg'] >= -3)), 'zscore_agg_class'] = 'regular zscore'
merged_inner_day.loc[(merged_inner_day['zscore_agg'] > 3) | (merged_inner_day['zscore_agg'] < -3), 'zscore_agg_class'] = 'bad zscore'

print("\nTotal merged registers: " + str(len(merged_inner_day)))
print("\nz-score <1 registers: " + str(len(merged_inner_day.loc[merged_inner_day['zscore_agg_class'] == 'good zscore'])))
print("z-score >1 & <3 registers: " + str(len(merged_inner_day.loc[merged_inner_day['zscore_agg_class'] == 'regular zscore'])))
print("z-score >3 registers: " + str(len(merged_inner_day.loc[merged_inner_day['zscore_agg_class'] == 'bad zscore'])))


fx = pltx.scatter(merged_inner_day.reset_index(), 'date', 'Temperature (C)', color='zscore_agg_class', color_discrete_map = color_discrete_map)
fx.update_xaxes (rangeslider_visible=True,)
fx.update_traces(marker_size=3)
fx.show()

print('\n Zscore >3 temperatures:')
print(merged_inner_day.loc[merged_inner_day['zscore_agg_class'] == 'bad zscore'])

"""##     Detecting outliers with machine learning algorithms

**Local Outlier Factor (LOF)**

Local Outlier Factor (LOF) algorithm is an unsupervised machine learning method intended for the detection of outliers.
This method is based on calculating the density deviation of a given datapoint with respect to its neighbors. It will be considered outliers all datapoints with lower density than their neighbors.

To analyze with this method, we will use sklearn.neighbors.LocalOutlierFactor library (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html).


To instance LOF we need to set two parameters:

- n_neighbors: number of neighbors to use for kneighbors queries (default value is 20)
- contamination: expected proportion of outliers in the dataset. The default value is 'auto', determined as in the original paper, otherwise should be in the range (0, 0.5].

After setting parameters we will instance fit_predict() method, returning for each datapoint a label 1 for an inlier or a label -1 for an outlier based on the LOF score and the contamination parameter set.
"""

from sklearn.neighbors import LocalOutlierFactor

X = df[['Temperature (C)']]
lof = LocalOutlierFactor(n_neighbors=100, contamination='auto')
df['LOF'] = lof.fit_predict(X)


df['LOF_class'] = 'normal'
df.loc[df['LOF'] < 1, 'LOF_class'] = 'LOF'

print('\n LOF outliers: ' + str(len(df[df['LOF'] < 1])))

color_discrete_map = {'normal': 'green', 'LOF': 'red'}
fx = pltx.scatter(df.reset_index(), 'date', 'Temperature (C)', color='LOF_class', color_discrete_map = color_discrete_map)
fx.update_xaxes (rangeslider_visible=True,)
fx.update_traces(marker_size=3)
fx.show()

"""**Isolation Forest**

IsolationForest is an unsupervised machine learning algorithm that identifies possible anomalies by isolating outliers in a dataset. Its calculation is inspired by the Random Forest classification and regression algorithm.

To analyze with this method, we will use sklearn.ensemble.IsolationForest library (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html).


To instance IsolationForest we need to set three parameters:
- n_estimators: number of base estimators or trees in the ensemble. It's optional and the default value is 100.
- max_samples: number of samples used to train each base estimator. The default value of max_samples is 'auto', max_samples=min(256, n_samples).
- contamination: expected proportion of outliers in the dataset. The default value is 'auto', determined as in the original paper of Isolation Forest.

"""

from sklearn.ensemble import IsolationForest

model = IsolationForest(n_estimators = 500, max_samples = 'auto', contamination=0.01)

model.fit(df[['Temperature (C)']])

df['IsolationForest_scores'] = model.decision_function(df[['Temperature (C)']])
df['IsolationForest_anomaly_score'] = model.predict(df[['Temperature (C)']])

df['IF_class'] = 'normal'
df.loc[df['IsolationForest_anomaly_score'] == -1, 'IF_class'] = 'outlier'

print('\n Isolation Forest outliers: ' + str(len(df[df['IsolationForest_anomaly_score'] == -1])))

color_discrete_map = {'normal': 'green', 'outlier': 'red'}
fx = pltx.scatter(df.reset_index(), 'date', 'Temperature (C)', color='IF_class', color_discrete_map = color_discrete_map)
fx.update_xaxes (rangeslider_visible=True,)
fx.update_traces(marker_size=3)
fx.show()
